<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Minicom.nl]]></title>
  <link href="http://blog.minicom.nl/atom.xml" rel="self"/>
  <link href="http://blog.minicom.nl/"/>
  <updated>2014-11-08T18:12:14+01:00</updated>
  <id>http://blog.minicom.nl/</id>
  <author>
    <name><![CDATA[Michael de Jong]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Continuous Deployment]]></title>
    <link href="http://blog.minicom.nl/blog/2014/09/19/an-introduction-to-continuous-deployment/"/>
    <updated>2014-09-19T20:15:00+02:00</updated>
    <id>http://blog.minicom.nl/blog/2014/09/19/an-introduction-to-continuous-deployment</id>
    <content type="html"><![CDATA[<p>We rely increasingly more on web services in our lives. We cannot think of our world without the web services of Google, Facebook, Twitter and many others. But as we rely more and more on these services, we are less accepting when such a service is temporarily offline - regardless of whether this was due to a failure or maintenance. This puts additional strain on teams which are responsible for improving and operating these services. Especially when a web service has to be updated to a newer version while still remaining online and available to its users. In this text we&rsquo;ll explore some of the ways to achieve Continuous Deployment and the challenges that come with it.</p>

<h2>What is Continuous Deployment?</h2>

<p>In recent years development teams have been transitioning to alternative methods of development. Moving towards more Agile methodologies and concepts they are now able to produce production-ready deliverables on a continuous basis. These deliverables are automatically - and continuously - tested using an automated unit- and integration test suite. By being able to produce these production-ready deliverables in quick succession, these development teams are able to quickly adapt to customer requirements and are quick to deal with bugs. But being able to produce these deliverables in a continuous pace is not enough to fulfill customer requirements. These deliverables must also be deployed into production for customers to benefit from these improvements. This is where Continuous Deployment comes into view.</p>

<p>The goal of Continuous Deployment is to ensure that (almost) every production-ready deliverable is automatically deployed to all production servers, thereby replacing the older version. There are various tools, techniques and strategies to achieve this, each with their own advantages and disadvantages. In this post I will focus only on this upgrade process.</p>

<h2>The Art of Load balancing</h2>

<p>Deploying a new version of a web service without causing downtime is typically done using some form of load balancing. A load balancer is a piece of software which sits between the various servers running the web service - called a server pool - and the connection to the internet. A load balancer is able to redirect incoming user requests to one particular server in the server pool based on a set of rules. By changing these rules, one is able to remove a server from the server pool - effectively not letting that server process anymore user requests, or adding a server to the server pool - which will cause some of the traffic to be redirected to that server. There are two strategies available using a load balancer which can help teams deploy new versions of their web service without the need to go offline. These are known as &ldquo;rolling upgrades&rdquo; and &ldquo;big-flips&rdquo; (also known as atomic switches).</p>

<h3>Rolling upgrades</h3>

<p>The typical setup for rolling upgrades is one (or more) load balancers redirecting incoming user traffic to a set of servers. This group of servers is called a server pool. When you wish to upgrade a server, you instruct the loadbalancer to remove that server from the server pool. In essence this means that the server will no longer receive any traffic coming through the load balancer from users. You can then gracefully terminate, upgrade, and restart the service(s) installed on the server safely. When these steps have been completed, the server can be re-added to the server pool. As a result the server will then start receiving traffic again through the load balancer. This procedure completes when every server has been taken from the server pool, is upgraded, and then re-added to the server pool.
<img src="http://blog.minicom.nl/images/rolling-upgrade.png" alt="" /></p>

<p>To avoid a significant loss of capacity or performance you can choose to upgrade one server at a time. This is usefull for small environments with few servers where there is little over-provisioning of computing resources. Alternatively you can also choose to upgrade several servers concurrently, which will take less time to complete the entire upgrade, but will adversely affect your capacity and maybe even performance of your web service.</p>

<h3>Big-flips</h3>

<p>Another option is the use of big-flips (also known as atomic switches). With big-flips you need a different setup of your servers. You will typically have a load balancer which redirects incoming user traffic to a server pool much like with rolling upgrades, but when you wish to perform an upgrade to your service you will need a duplicate server pool with equivalent resources to take over the workload from the initial server pool. So in order to perform the actual upgrade you must first install the upgraded version of the service on the alternate server pool. When that has been done, you will need to instruct the load balancer to stop redirecting traffic to the initial server pool, and instead redirect it to the alternate server pool. You can then safely terminate the services running on the initial server pool and release the computational resources used in this server pool.
<img src="http://blog.minicom.nl/images/big-flip.png" alt="" /></p>

<p>Big-flips are very costly in terms of computational resources. For every upgrade you require a duplicate of your production environment, and only for a very short period of time when you are performing the upgrade. This can partially be offset by using cloud providers which will rent you these computational resources per minute or hour and allow you to dispose of the initial server pool once the upgrade has succesfully completed.</p>

<h3>Trade offs</h3>

<p>I&rsquo;ve briefly introduced you to two different strategies you can use to deploy a new version of a web service. You might have noticed that these two have two very different types of &ldquo;cost&rdquo; and &ldquo;advantage&rdquo;. While rolling upgrades are slow when doing an entire upgrade sequentially for every server, they are very effecient in terms of computational resources - they require very little over-provisioning of resources, whereas big flips are quicker to perform the upgrade they require double the computational resources, which can be (too) costly.</p>

<p>So in a sense these two strategies sit at opposite ends of the same trade off: Effecient use of computational resources or a faster upgrade process.</p>

<h2>The challenge of mixed-state</h2>

<p>An often ignored part of achieving Continuous Deployment is the term &ldquo;Mixed-state&rdquo;. No upgrade or deployment is immune to it, and has to come up with some way of dealing with it. When you envision the entire web service - running on multiple servers - as a distributed software system, you need to ensure that every separate part can interface correctly with its dependencies. Mixed-state refers to the situation where mid-upgrade or mid-deployment you have two versions of the same software running concurrently, using the same dependencies. In a sense, the distributed software system is in two states now. This means that any dependency on which both versions operate, must be compatible with both versions. This is typically solved using versioned and static APIs which allow dependencies to communicate amongst each other without problems. There is however one notable exception: databases.</p>

<p>Imagine running two versions of the same web service on the same database, which has stored all its persistent data in some structure or schema. If the never version requires a slightly different schema, the older version might cease to function which could result in breaking the web service, until the upgrade of the system has completed and moved from a mixed-state to a single state again. This is aggrevated when the upgrade process takes a long time to complete.</p>

<p>This is an ongoing challenge, there is no off the shelf solution, and in this upcoming blog series I will to cover it more in-depth. I will be showing you some of my research and results in this area. Stay tuned for more&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Contributing to RxJava]]></title>
    <link href="http://blog.minicom.nl/blog/2013/07/14/contributing-to-rxjava/"/>
    <updated>2013-07-14T22:54:00+02:00</updated>
    <id>http://blog.minicom.nl/blog/2013/07/14/contributing-to-rxjava</id>
    <content type="html"><![CDATA[<p>I&rsquo;m a MSc Computer Science student at the Delft University of Technology. During my master track I&rsquo;ve had the pleasure to have followed two courses given by Erik Meijer (former Microsoft employee). Erik was involved in the creation of the Rx framework in Microsoft&rsquo;s .NET C#. If you&rsquo;re not familiar with Rx, I would recommend some of the videos found on <a href="http://channel9.msdn.com/tags/Rx/">Channel9</a>.</p>

<p>One of the courses Erik has been teaching is Reactive Programming where he covers the Rx framework and the thoughts behind it. As a part of the course, we were asked to contribute to the Rx ecosystem in whatever programming language we liked. I choose to contribute to Netflix&rsquo;s RxJava. Last week I presented my contributions and experiences to my fellow students. Below you can find the presentation slides I used.</p>

<script async class="speakerdeck-embed" data-id="c50bca20cee7013097764230e68ed5e3" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automated web application testing for humans]]></title>
    <link href="http://blog.minicom.nl/blog/2013/04/27/automated-web-application-testing-for-humans/"/>
    <updated>2013-04-27T22:10:00+02:00</updated>
    <id>http://blog.minicom.nl/blog/2013/04/27/automated-web-application-testing-for-humans</id>
    <content type="html"><![CDATA[<p>Web applications have become a very popular breed of software. The fact that the software needed to use a web application is already installed on every internet-capable computer adds to its appeal. For a software developer there are many different architectures and even more frameworks to choose from to build web applications with. But one important part of software development - testing - can be very hard in web applications.</p>

<p>There are a few tools out there to help you with testing web applications. The most popular one seems to be Selenium. Selenium offers you two options for creating automated tests for web applications: <a href="http://docs.seleniumhq.org/projects/ide/">Selenium IDE</a>, and <a href="http://docs.seleniumhq.org/projects/webdriver/">Selenium WebDriver</a>. Selenium IDE is a simple plugin for FireFox which enables you to record interactions with a browser and replay those interactions. It&rsquo;s most suitable for testers without a programming background. Selenium WebDriver on the other hand allows testers with a programming background (or developers) to write their own automated tests, which can be run from JUnit tests. <strong>But while using these tools myself I found myself easily annoyed at how fragile these tests actually are.</strong> In order to understand why this is so you have to understand how Selenium works.</p>

<p>When you use Selenium IDE to record a session of interactions with your browser it&rsquo;s actually looking at what events you are triggering on which elements in the Document Object Model (DOM). Most of these elements can be identified with a <code>name</code>, <code>id</code> or <code>class</code> attribute. Take for example the login screen of Google&rsquo;s Gmail service. The username and password field are aptly named &ldquo;Email&rdquo; and &ldquo;Passwd&rdquo; in the DOM using the <code>id</code> attribute. So when we make a recording using Selenium IDE, it will know that I&rsquo;m typing in those two fields, and store the interaction together with that id attribute.</p>

<p><img src="http://blog.minicom.nl/images/gmail-login.png" alt="img" /> <img src="http://blog.minicom.nl/images/selenium-ide.png" alt="img" /></p>

<p>Although this is a fairly cheap way to construct a test, there are several problems with this approach:</p>

<ul>
<li><strong>Tests are not necessarily repeatable</strong>: Web application frameworks like ExtJS and Google Web Toolkit produce different unique <code>id</code> attribute values on every page serve. This means we need an alternative way of selecting the correct element in the DOM. There are several options like CSS and XPath selectors but these can get fairly complicated very quickly, which in turn negatively affects readability.</li>
<li><strong>Tests are easily broken</strong>: When a developer or designer changes the structure of the DOM, there is a chance that certain CSS and XPath selectors will no longer work. Even worse, in case the name of the <code>id</code> attribute is changed the test will definitely fail and it will have to be fixed.</li>
<li><strong>Repairing tests means re-recording the interaction between user and browser</strong>: The cost of recording is relatively small, but if tests break regularly this can become prohibitively expensive. Imaging that most of your tests are done somewhere in a secured area of your web application. In order to get there, the browser first has to login into the web application. If this login page is modified, all of these tests will have to be either re-recorded or fixed manually.</li>
<li><strong>Test setup and tear down is expensive</strong>: In case you want to test a feature in your web application you are first required to log in, navigate to the appropriate page, perform the actual test, and log out. Such a test case can take considerable time to record, while the only important section is the actual test.</li>
</ul>


<p>There is an obvious advantage in having an automated test suite which tests your web application from a user&rsquo;s perspective, but these issues are making automated tests unwieldy and costly. The most sensible conclusion is that the approach of working on a DOM level is simply too low-level for creating and maintaining an extensive automated test suite. But now let&rsquo;s approach this from a tester&rsquo;s point of view. Most of the testers I know are very capable at writing test cases for manual regression testing. These test cases are well documented and manually executed before pushing software out to customers. These regression test documents usually have the following structure:</p>

<blockquote><p><strong>Preconditions</strong>: Your browser is displaying the login screen.</p>

<p><strong>Procedure</strong></p>

<ol>
<li>Enter &ldquo;admin&rdquo; in the &ldquo;Username&rdquo; field.</li>
<li>Enter &ldquo;test&rdquo; in the &ldquo;Password&rdquo; field.</li>
<li>Click on the &ldquo;Login&rdquo; button.</li>
</ol>


<p><strong>Postcondition</strong>: You are now logged in, and your browser should be displaying the user&rsquo;s dashboard.</p></blockquote>

<p>These regression test documents have a clear structure and are easy to read and maintain for testers. The reason that this works for human testers is that they can reason about what&rsquo;s displayed in the browser. They know that the &ldquo;Username&rdquo; field is the field that has the text &ldquo;Username&rdquo; displayed right next to it. They know that the &ldquo;Login&rdquo; button is the element that&rsquo;s clickable and contains the text &ldquo;Login&rdquo;. They know what the user&rsquo;s dashboard looks like. My ideal test automation tool for web applications would be a tool which is capable of interpreting test documents like to one above, and execute them fully automatic.</p>

<p>If you iterate through all the issues I named earlier, you&rsquo;ll notice that such a tool could easily address the first three issues in the list. Since test documents don&rsquo;t specifically name attributes of elements in the DOM, it doesn&rsquo;t really matter what the <code>id</code> attribute of any element says. Modifications to the DOM on a structural level don&rsquo;t affect the interpretation of the presented web page to the same degree. And finally because this type of &lsquo;recording&rsquo; is much more resilient to changes in the DOM, it should require only a minimal amount of test case maintenance, whilst the test cases themselves remain human-readable.</p>

<p>I&rsquo;m very interested to see if it would be possible to create such a tool and to see if it indeed holds so much value for testers as I believe that it does&hellip;</p>
]]></content>
  </entry>
  
</feed>
